import numpy as np
import pandas as pd
import os

try:
    from app.data_handler import load_csv
except ImportError:
    print("CRITICAL ERROR: Could not import 'load_csv' from 'app.data_handler'.")
    raise

from .helpers import load_normalization_json, denormalize, verify_date_consistency
from .sliding_windows import SlidingWindowsProcessor
from .target_calculation import TargetCalculationProcessor
from .anti_naive_lock import AntiNaiveLockProcessor


class PreprocessorPlugin:
    """
    Preprocessor implementing the exact required flow:
    1. Load normalized CSV data (keep as-is)
    2. Denormalize all input datasets using JSON parameters
    3. Create sliding windows from denormalized data
    4. Extract baselines (last elements of each window for target column)
    5. Calculate log return targets with those baselines
    6. Apply anti-naive-lock transformations to denormalized input datasets
    7. Create final sliding windows matrix from processed datasets
    """
    
    plugin_params = {
        "x_train_file": "data/x_train.csv",
        "y_train_file": "data/y_train.csv",
        "x_validation_file": "data/x_val.csv",
        "y_validation_file": "data/y_val.csv",
        "x_test_file": "data/x_test.csv",
        "y_test_file": "data/y_test.csv",
        "headers": True,
        "max_steps_train": None, 
        "max_steps_val": None, 
        "max_steps_test": None,
        "target_column": "TARGET",
        "window_size": 48,
        "predicted_horizons": [24, 48, 72, 96, 120, 144],
        "use_returns": True,
        "normalize_features": True,
        "anti_naive_lock_enabled": True,
        "feature_preprocessing_strategy": "selective",
    }
    
    plugin_debug_vars = [
        "window_size", "predicted_horizons", "use_returns", "normalize_features",
        "target_returns_means", "target_returns_stds"
    ]

    def __init__(self):
        self.params = self.plugin_params.copy()
        self.scalers = {}
        self.sliding_windows_processor = SlidingWindowsProcessor(self.scalers)
        self.target_calculation_processor = TargetCalculationProcessor()
        self.anti_naive_lock_processor = AntiNaiveLockProcessor()

    def set_params(self, **kwargs):
        for key, value in kwargs.items(): 
            self.params[key] = value

    def get_debug_info(self):
        debug_info = {}
        for var in self.plugin_debug_vars:
            value = self.params.get(var)
            debug_info[var] = value
        return debug_info

    def add_debug_info(self, debug_info):
        debug_info.update(self.get_debug_info())

    def _load_data(self, file_path, max_rows, headers, config):
        """Load CSV data and KEEP IT NORMALIZED for neural network training."""
        print(f"Loading data: {file_path} (Max rows: {max_rows})...", end="")
        try:
            df = load_csv(file_path, headers=headers, max_rows=max_rows)
            if df is None or df.empty: 
                raise ValueError(f"load_csv None/empty for {file_path}")
            print(f" Done. Shape: {df.shape}")
            
            # Handle datetime index conversion
            if not isinstance(df.index, pd.DatetimeIndex):
                print(f"Attempting DatetimeIndex conversion for {file_path}...", end="")
                original_index_name = df.index.name
                try: 
                    df.index = pd.to_datetime(df.index)
                    print(" OK (from index).")
                except Exception:
                    try: 
                        df.index = pd.to_datetime(df.iloc[:, 0])
                        print(" OK (from col 0).")
                    except Exception as e_col: 
                        print(f" FAILED ({e_col}). Dates unavailable.")
                        df.index = None
                if original_index_name: 
                    df.index.name = original_index_name
            
            # Validate required columns
            required_cols = ["CLOSE"]
            target_col_name = self.params.get("target_column", "TARGET")
            if 'y_' in os.path.basename(file_path).lower(): 
                required_cols.append(target_col_name)
            missing_cols = [c for c in required_cols if c not in df.columns]
            if missing_cols: 
                raise ValueError(f"Missing cols in {file_path}: {missing_cols}")
            
            # ðŸ”‘ CRITICAL FIX: KEEP DATA NORMALIZED FOR NEURAL NETWORK TRAINING
            print(f"âœ… KEEPING NORMALIZED DATA: Input data is already z-score normalized and ready for neural networks")
            norm_json = load_normalization_json(config)
            
            # Verify normalization by checking a few key features
            for column in ['CLOSE', 'RSI', 'MACD']:
                if column in df.columns:
                    col_data = df[column].values
                    col_mean = np.mean(col_data)
                    col_std = np.std(col_data)
                    print(f"  ï¿½ {column}: mean={col_mean:.3f}, std={col_std:.3f} (normalized scale)")
                    
                    # Check if data appears properly normalized (should be roughly mean~0, std~1 or similar small scale)
                    if abs(col_mean) > 100 or col_std > 100:
                        print(f"    âš ï¸ WARNING: {column} may not be properly normalized (large values detected)")
                    else:
                        print(f"    âœ… {column} appears properly normalized for neural network training")
            
            print(f"  âœ… Data kept in normalized state for optimal neural network training")
            return df
            
        except FileNotFoundError: 
            print(f"\nERROR: File not found: {file_path}.")
            raise
        except Exception as e: 
            print(f"\nERROR loading/processing {file_path}: {e}")
            import traceback
            traceback.print_exc()
            raise

    def _align_indices(self, x_train_df, y_train_df, x_val_df, y_val_df, x_test_df, y_test_df):
        """Align indices between X and Y dataframes for all splits."""
        print("\n--- 2. Aligning Indices ---")
        
        aligned_data = {}
        for split, x_df, y_df in [("Train", x_train_df, y_train_df), 
                                 ("Validation", x_val_df, y_val_df), 
                                 ("Test", x_test_df, y_test_df)]:
            
            # Align datetime indices if both are DatetimeIndex
            if isinstance(x_df.index, pd.DatetimeIndex) and isinstance(y_df.index, pd.DatetimeIndex):
                if not x_df.index.equals(y_df.index):
                    print(f"WARN: {split} X/Y index misalignment. X: {x_df.index[0]} to {x_df.index[-1]}, Y: {y_df.index[0]} to {y_df.index[-1]}")
                    common_index = x_df.index.intersection(y_df.index)
                    if len(common_index) == 0:
                        raise ValueError(f"No common indices between X and Y for {split} split.")
                    print(f"Aligning {split} to common indices: {len(common_index)} rows.")
                    x_df = x_df.loc[common_index]
                    y_df = y_df.loc[common_index]
            
            # Align lengths
            if len(x_df) != len(y_df):
                min_len = min(len(x_df), len(y_df))
                print(f"WARN: {split} X/Y length mismatch. X: {len(x_df)}, Y: {len(y_df)}. Truncating to {min_len}.")
                x_df = x_df.iloc[:min_len]
                y_df = y_df.iloc[:min_len]
            
            # Store aligned data
            split_key = split.lower().replace('validation', 'val')
            aligned_data[f'x_{split_key}_df'] = x_df
            aligned_data[f'y_{split_key}_df'] = y_df
        
        return aligned_data

    def _prepare_baseline_data(self, aligned_data, config):
        """Prepare baseline data from aligned normalized dataframes."""
        print("\n--- 3. Preparing Baseline Data ---")
        
        norm_json = load_normalization_json(config)
        baseline_data = {'norm_json': norm_json}
        
        target_column = config.get("target_column", "CLOSE")
        window_size = config.get("window_size", 48)
        
        splits = ['train', 'val', 'test']
        for split in splits:
            x_df = aligned_data[f'x_{split}_df']
            y_df = aligned_data[f'y_{split}_df']
            
            # Store NORMALIZED dataframes (input data is z-score normalized)
            baseline_data[f'x_{split}_df'] = x_df  # These are normalized
            baseline_data[f'y_{split}_df'] = y_df  # These are normalized
            
            # CLOSE prices are NORMALIZED (from input CSV) - will be denormalized for sliding windows
            close_normalized = x_df["CLOSE"].astype(np.float32).values
            baseline_data[f'close_{split}'] = close_normalized  # Store normalized data
            
            # Extract target column - NORMALIZED from input CSV
            target_from_y_normalized = y_df[target_column].astype(np.float32).values
            
            # Trim first window_size-1 elements to align with sliding windows
            target_from_y_trimmed = target_from_y_normalized[window_size-1:]
            baseline_data[f'target_baseline_{split}'] = target_from_y_trimmed
            
            # Extract and trim dates
            dates = x_df.index if isinstance(x_df.index, pd.DatetimeIndex) else None
            if dates is not None:
                dates_trimmed = dates[window_size-1:]
                baseline_data[f'dates_{split}'] = dates_trimmed
            else:
                baseline_data[f'dates_{split}'] = None
            
            print(f"{split.capitalize()} baseline prepared: {len(close_normalized)} samples (normalized), target baseline: {len(target_from_y_trimmed)} samples (normalized)")
        
        return baseline_data

    def _truncate_to_match_targets(self, windowed_data, target_data):
        """Truncate windowed data to match target data lengths."""
        print("\n--- Final Length Alignment ---")
        
        splits = ['train', 'val', 'test']
        for split in splits:
            # Get target data length for this split (all horizons should have same length now)
            if f'y_{split}' in target_data and target_data[f'y_{split}']:
                # Get the length from the first horizon
                first_horizon_key = list(target_data[f'y_{split}'].keys())[0]
                target_length = len(target_data[f'y_{split}'][first_horizon_key])
                
                # Verify all horizons have the same length
                for horizon_key, horizon_data in target_data[f'y_{split}'].items():
                    if len(horizon_data) != target_length:
                        print(f"WARNING: Inconsistent target lengths in {split}: {horizon_key} has {len(horizon_data)}, expected {target_length}")
                
                # Truncate windowed features to match target length
                X_key = f'X_{split}'
                if X_key in windowed_data and len(windowed_data[X_key]) > target_length:
                    print(f"Truncating {split} X data from {len(windowed_data[X_key])} to {target_length} samples")
                    windowed_data[X_key] = windowed_data[X_key][:target_length]
                    
                    # Also truncate dates
                    dates_key = f'x_dates_{split}'
                    if dates_key in windowed_data and windowed_data[dates_key] is not None:
                        if len(windowed_data[dates_key]) > target_length:
                            windowed_data[dates_key] = windowed_data[dates_key][:target_length]
                    
                    # Update sample count
                    windowed_data[f'num_samples_{split}'] = target_length
            else:
                print(f"No target data found for {split} split")

    def process_data(self, config):
        """
        Exact implementation of required flow:
        1. Load normalized CSV data (keep as-is)
        2. Denormalize all input datasets using JSON parameters
        3. Create sliding windows from denormalized data
        4. Extract baselines (last elements of each window for target column)
        5. Calculate log return targets with those baselines
        6. Apply anti-naive-lock transformations to denormalized input datasets
        7. Create final sliding windows matrix from processed datasets
        """
        print("Starting preprocessing pipeline")
        self.set_params(**config)
        config = self.params
        
        predicted_horizons = config['predicted_horizons']
        if not isinstance(predicted_horizons, list) or not predicted_horizons: 
            raise ValueError("'predicted_horizons' must be a non-empty list.")
        
        # 1. Load normalized CSV data (keep as-is)
        print("Step 1: Loading normalized CSV data")
        x_train_df = self._load_data(config["x_train_file"], config.get("max_steps_train"), config.get("headers"), config)
        x_val_df = self._load_data(config["x_validation_file"], config.get("max_steps_val"), config.get("headers"), config)
        x_test_df = self._load_data(config["x_test_file"], config.get("max_steps_test"), config.get("headers"), config)
        y_train_df = self._load_data(config["y_train_file"], config.get("max_steps_train"), config.get("headers"), config)
        y_val_df = self._load_data(config["y_validation_file"], config.get("max_steps_val"), config.get("headers"), config)
        y_test_df = self._load_data(config["y_test_file"], config.get("max_steps_test"), config.get("headers"), config)
        
        aligned_data = self._align_indices(x_train_df, y_train_df, x_val_df, y_val_df, x_test_df, y_test_df)
        
        # 2. Denormalize all input datasets using JSON parameters
        print("Step 2: Denormalizing input datasets using JSON parameters")
        norm_json = load_normalization_json(config)
        
        denorm_data = {}
        for key, df in aligned_data.items():
            denorm_df = df.copy()
            for column in denorm_df.columns:
                if column in norm_json:
                    denorm_df[column] = denormalize(df[column].values, norm_json, column)
            denorm_data[key] = denorm_df
        
        # 3. Create sliding windows from denormalized data
        print("Step 3: Creating sliding windows from denormalized data")
        baseline_data = {'norm_json': norm_json}
        baseline_data.update(denorm_data)
        
        for split in ['train', 'val', 'test']:
            x_df = denorm_data[f'x_{split}_df']
            baseline_data[f'dates_{split}'] = x_df.index if isinstance(x_df.index, pd.DatetimeIndex) else None
        
        windowed_data = self.sliding_windows_processor.generate_windowed_features(baseline_data, config)
        
        # 4. Extract baselines (last elements of each window for target column)
        # 5. Calculate log return targets with those baselines
        print("Steps 4-5: Extracting baselines and calculating targets")
        target_data = self.target_calculation_processor.calculate_targets(baseline_data, windowed_data, config)
        
        # 6. Apply anti-naive-lock transformations to denormalized input datasets
        print("Step 6: Applying anti-naive-lock transformations to denormalized datasets")
        if config.get("anti_naive_lock_enabled", True):
            processed_data = {}
            for split in ['train', 'val', 'test']:
                x_df = denorm_data[f'x_{split}_df'] 
                y_df = denorm_data[f'y_{split}_df']
                
                feature_names = list(x_df.columns)
                x_values = x_df.values.reshape(1, len(x_df), len(feature_names))
                
                x_processed, _, _, _ = self.anti_naive_lock_processor.process_sliding_windows(
                    x_values, x_values, x_values, feature_names, config
                )
                
                processed_x_df = pd.DataFrame(
                    x_processed[0], 
                    index=x_df.index, 
                    columns=feature_names
                )
                
                processed_data[f'x_{split}_df'] = processed_x_df
                processed_data[f'y_{split}_df'] = y_df
        else:
            processed_data = denorm_data
        
        # 7. Create final sliding windows matrix from processed datasets
        print("Step 7: Creating final sliding windows from processed datasets")
        final_data = {'norm_json': norm_json}
        final_data.update(processed_data)
        
        for split in ['train', 'val', 'test']:
            x_df = processed_data[f'x_{split}_df']
            final_data[f'dates_{split}'] = x_df.index if isinstance(x_df.index, pd.DatetimeIndex) else None
        
        final_windowed_data = self.sliding_windows_processor.generate_windowed_features(final_data, config)
        
        self._truncate_to_match_targets(final_windowed_data, target_data)
        
        return {
            "x_train": final_windowed_data['X_train'],
            "x_val": final_windowed_data['X_val'], 
            "x_test": final_windowed_data['X_test'],
            
            "y_train": target_data['y_train'],
            "y_val": target_data['y_val'],
            "y_test": target_data['y_test'],
            
            "x_train_dates": final_windowed_data['x_dates_train'],
            "y_train_dates": final_windowed_data['x_dates_train'],
            "x_val_dates": final_windowed_data['x_dates_val'],
            "y_val_dates": final_windowed_data['x_dates_val'],
            "x_test_dates": final_windowed_data['x_dates_test'],
            "y_test_dates": final_windowed_data['x_dates_test'],
            
            "baseline_train": target_data.get('baseline_train', np.array([])),
            "baseline_val": target_data.get('baseline_val', np.array([])),
            "baseline_test": target_data.get('baseline_test', np.array([])),
            
            "feature_names": final_windowed_data['feature_names'],
            "target_returns_means": target_data['target_returns_means'],
            "target_returns_stds": target_data['target_returns_stds'],
            "predicted_horizons": predicted_horizons,
            "normalization_json": norm_json,
        }

    def run_preprocessing(self, config):
        """Run preprocessing with given configuration."""
        run_config = self.params.copy()
        run_config.update(config)
        self.set_params(**run_config)
        processed_data = self.process_data(self.params)
        
        # Include target normalization parameters in returned params
        params_with_targets = self.params.copy()
        params_with_targets.update({
            "target_returns_means": processed_data.get("target_returns_means", []),
            "target_returns_stds": processed_data.get("target_returns_stds", []),
            "training_norm_params": {},  # No additional normalization applied
            "normalization_json": processed_data.get("normalization_json", {})  # Original normalization
        })
        
        return processed_data, params_with_targets